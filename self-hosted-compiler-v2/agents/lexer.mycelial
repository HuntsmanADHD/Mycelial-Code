    hyphal lexer {
      state {
        source: string                    # Source code to tokenize
        source_len: u32                   # Cached source length (for O(1) access)
        filename: string                  # Source filename
        position: u32                     # Current position in source
        line: u32                         # Current line number
        column: u32                       # Current column number
        tokens_emitted: u32               # Count of tokens emitted
        error_count: u32                  # Lexer error count

        # Keyword lookup table
        keywords: map<string, TokenType>     # Word -> Token type mapping
      }

      # Initialize lexer state
      on rest {
        state.position = 0
        state.line = 1
        state.column = 1
        state.tokens_emitted = 0
        state.error_count = 0

        # Initialize keywords map
        state.keywords = map_new()
        map_insert(state.keywords, "network", TokenType::NETWORK)
        map_insert(state.keywords, "frequencies", TokenType::FREQUENCIES)
        map_insert(state.keywords, "frequency", TokenType::FREQUENCY)
        map_insert(state.keywords, "hyphae", TokenType::HYPHAE)
        map_insert(state.keywords, "hyphal", TokenType::HYPHAL)
        map_insert(state.keywords, "state", TokenType::STATE)
        map_insert(state.keywords, "on", TokenType::ON)
        map_insert(state.keywords, "signal", TokenType::SIGNAL)
        map_insert(state.keywords, "emit", TokenType::EMIT)
        map_insert(state.keywords, "report", TokenType::REPORT)
        map_insert(state.keywords, "spawn", TokenType::SPAWN)
        map_insert(state.keywords, "die", TokenType::DIE)
        map_insert(state.keywords, "socket", TokenType::SOCKET)
        map_insert(state.keywords, "fruiting_body", TokenType::FRUITING_BODY)
        map_insert(state.keywords, "topology", TokenType::TOPOLOGY)
        map_insert(state.keywords, "config", TokenType::CONFIG)
        map_insert(state.keywords, "if", TokenType::IF)
        map_insert(state.keywords, "else", TokenType::ELSE)
        map_insert(state.keywords, "where", TokenType::WHERE)
        map_insert(state.keywords, "rest", TokenType::REST)
        map_insert(state.keywords, "cycle", TokenType::CYCLE)
        map_insert(state.keywords, "let", TokenType::LET)
        map_insert(state.keywords, "for", TokenType::FOR)
        map_insert(state.keywords, "in", TokenType::IN)
        map_insert(state.keywords, "while", TokenType::WHILE)
        map_insert(state.keywords, "rule", TokenType::RULE)
        map_insert(state.keywords, "return", TokenType::RETURN)
        map_insert(state.keywords, "break", TokenType::BREAK)
        map_insert(state.keywords, "continue", TokenType::CONTINUE)
        map_insert(state.keywords, "match", TokenType::MATCH)
        map_insert(state.keywords, "as", TokenType::AS)
        map_insert(state.keywords, "import", TokenType::IMPORT)
        map_insert(state.keywords, "types", TokenType::TYPES)
        map_insert(state.keywords, "struct", TokenType::STRUCT)
        map_insert(state.keywords, "enum", TokenType::ENUM)
        map_insert(state.keywords, "constants", TokenType::CONSTANTS)
        map_insert(state.keywords, "true", TokenType::TRUE)
        map_insert(state.keywords, "false", TokenType::FALSE)
        map_insert(state.keywords, "u8", TokenType::U8)
        map_insert(state.keywords, "u16", TokenType::U16)
        map_insert(state.keywords, "u32", TokenType::U32)
        map_insert(state.keywords, "u64", TokenType::U64)
        map_insert(state.keywords, "i8", TokenType::I8)
        map_insert(state.keywords, "i16", TokenType::I16)
        map_insert(state.keywords, "i32", TokenType::I32)
        map_insert(state.keywords, "i64", TokenType::I64)
        map_insert(state.keywords, "f32", TokenType::F32)
        map_insert(state.keywords, "f64", TokenType::F64)
        map_insert(state.keywords, "string", TokenType::STRING_TYPE)
        map_insert(state.keywords, "binary", TokenType::BINARY_TYPE)
        map_insert(state.keywords, "boolean", TokenType::BOOLEAN_TYPE)
        map_insert(state.keywords, "vec", TokenType::VEC)
        map_insert(state.keywords, "queue", TokenType::QUEUE)
        map_insert(state.keywords, "map", TokenType::MAP)
      }

      # Receive tokenization request
      on signal(lex_request, req) {
        state.source = req.source
        state.source_len = string_len(req.source)  # Cache length for O(1) access
        state.filename = req.filename
        state.position = 0
        state.line = 1
        state.column = 1
        state.tokens_emitted = 0
        state.error_count = 0

        # Start tokenization
        tokenize_all()
      }

      # Main tokenization loop
      rule tokenize_all() {
        while state.position < state.source_len {
          skip_whitespace_and_comments()

          if state.position >= state.source_len {
            break
          }

          let tok = next_token()
          emit token {
            type: tok.type,
            value: tok.value,
            line: tok.line,
            column: tok.column
          }
          state.tokens_emitted = state.tokens_emitted + 1
        }

        # Emit EOF token
        emit token {
          type: TokenType::EOF,
          value: "",
          line: state.line,
          column: state.column
        }
        state.tokens_emitted = state.tokens_emitted + 1

        # Signal completion
        emit lex_complete {
          token_count: state.tokens_emitted,
          error_count: state.error_count
        }
      }

      # Peek at current character
      rule peek(offset: u32) -> string {
        let pos = state.position + offset
        if pos >= state.source_len {
          return ""
        }
        return string_char_at(state.source, pos)
      }

      # Advance position and return current character
      rule advance() -> string {
        if state.position >= state.source_len {
          return ""
        }

        let ch = string_char_at(state.source, state.position)
        state.position = state.position + 1

        if ch == "\n" {
          state.line = state.line + 1
          state.column = 1
        } else {
          state.column = state.column + 1
        }

        return ch
      }

      # Skip whitespace
      rule skip_whitespace() {
        while state.position < state.source_len {
          let ch = peek(0)
          if ch == " " || ch == "\t" || ch == "\n" || ch == "\r" {
            advance()
          } else {
            break
          }
        }
      }

      # Skip comment (# to end of line)
      rule skip_comment() -> boolean {
        if peek(0) == "#" {
          while state.position < state.source_len && peek(0) != "\n" {
            advance()
          }
          if peek(0) == "\n" {
            advance()
          }
          return true
        }
        return false
      }

      # Skip whitespace and comments
      rule skip_whitespace_and_comments() {
        while true {
          skip_whitespace()
          if !skip_comment() {
            break
          }
        }
      }

      # Read a number (integer or float, including hex literals)
      rule read_number() -> Token {
        let start_line = state.line
        let start_col = state.column
        let num_str = ""

        # Check for hex literal: 0x...
        if peek(0) == "0" && (peek(1) == "x" || peek(1) == "X") {
          num_str = string_concat(num_str, advance())  # consume '0'
          num_str = string_concat(num_str, advance())  # consume 'x'

          # Read hex digits
          while state.position < state.source_len && is_hex_digit(peek(0)) {
            num_str = string_concat(num_str, advance())
          }

          return Token {
            type: TokenType::NUMBER,
            value: num_str,
            line: start_line,
            column: start_col
          }
        }

        # Read integer part
        while state.position < state.source_len && is_digit(peek(0)) {
          num_str = string_concat(num_str, advance())
        }

        # Read decimal part if present
        if peek(0) == "." && is_digit(peek(1)) {
          num_str = string_concat(num_str, advance())  # consume '.'
          while state.position < state.source_len && is_digit(peek(0)) {
            num_str = string_concat(num_str, advance())
          }
        }

        # Read optional type suffix (u8, u16, u32, u64, i8, i16, i32, i64, f32, f64)
        let suffix_char = peek(0)
        if suffix_char == "u" || suffix_char == "i" || suffix_char == "f" {
          let suffix_start = num_str
          let suffix = suffix_char
          advance()  # consume u/i/f

          # Read digits after prefix
          while state.position < state.source_len && is_digit(peek(0)) {
            suffix = string_concat(suffix, advance())
          }

          # Check if it's a valid type suffix
          if suffix == "u8" || suffix == "u16" || suffix == "u32" || suffix == "u64" ||
             suffix == "i8" || suffix == "i16" || suffix == "i32" || suffix == "i64" ||
             suffix == "f32" || suffix == "f64" {
            num_str = string_concat(num_str, suffix)
          }
          # If not valid, we already consumed the chars - they'll be re-tokenized
          # This won't happen in well-formed code
        }

        return Token {
          type: TokenType::NUMBER,
          value: num_str,
          line: start_line,
          column: start_col
        }
      }

      # Read a string literal
      rule read_string(quote: string) -> Token {
        let start_line = state.line
        let start_col = state.column
        let str_val = ""

        advance()  # consume opening quote

        while state.position < state.source_len && peek(0) != quote {
          if peek(0) == "\\" {
            advance()  # consume backslash
            let next_ch = advance()
            if next_ch == "n" {
              str_val = string_concat(str_val, "\n")
            } else if next_ch == "t" {
              str_val = string_concat(str_val, "\t")
            } else if next_ch == "r" {
              str_val = string_concat(str_val, "\r")
            } else if next_ch == "\\" {
              str_val = string_concat(str_val, "\\")
            } else if next_ch == "\"" {
              str_val = string_concat(str_val, "\"")
            } else if next_ch == "'" {
              str_val = string_concat(str_val, "'")
            } else {
              str_val = string_concat(str_val, next_ch)
            }
          } else {
            str_val = string_concat(str_val, advance())
          }
        }

        if state.position >= state.source_len {
          state.error_count = state.error_count + 1
          emit compilation_error {
            stage: "lexer",
            message: "Unterminated string literal",
            line: start_line,
            column: start_col
          }
        } else {
          advance()  # consume closing quote
        }

        return Token {
          type: TokenType::STRING_LIT,
          value: str_val,
          line: start_line,
          column: start_col
        }
      }

      # Read an identifier or keyword
      rule read_identifier() -> Token {
        let start_line = state.line
        let start_col = state.column
        let ident = ""

        while state.position < state.source_len && is_alnum_or_underscore(peek(0)) {
          ident = string_concat(ident, advance())
        }

        # Check if it's a keyword
        if map_contains(state.keywords, ident) {
          return Token {
            type: map_get(state.keywords, ident),
            value: ident,
            line: start_line,
            column: start_col
          }
        }

        return Token {
          type: TokenType::IDENTIFIER,
          value: ident,
          line: start_line,
          column: start_col
        }
      }

      # Get next token
      rule next_token() -> Token {
        let start_line = state.line
        let start_col = state.column
        let ch = peek(0)

        # Numbers
        if is_digit(ch) {
          return read_number()
        }

        # Strings
        if ch == "\"" || ch == "'" {
          return read_string(ch)
        }

        # Identifiers and keywords
        if is_alpha_or_underscore(ch) {
          return read_identifier()
        }

        # Two-character operators
        let peek1 = peek(1)
        let two_char = string_concat(ch, peek1)

        if two_char == "->" {
          advance()
          advance()
          return Token { type: TokenType::ARROW, value: "->", line: start_line, column: start_col }
        }

        if two_char == "=>" {
          advance()
          advance()
          return Token { type: TokenType::FAT_ARROW, value: "=>", line: start_line, column: start_col }
        }

        if two_char == "==" {
          advance()
          advance()
          return Token { type: TokenType::EQ, value: "==", line: start_line, column: start_col }
        }

        if two_char == "!=" {
          advance()
          advance()
          return Token { type: TokenType::NE, value: "!=", line: start_line, column: start_col }
        }

        if two_char == "<=" {
          advance()
          advance()
          return Token { type: TokenType::LE, value: "<=", line: start_line, column: start_col }
        }

        if two_char == ">=" {
          advance()
          advance()
          return Token { type: TokenType::GE, value: ">=", line: start_line, column: start_col }
        }

        if two_char == "&&" {
          advance()
          advance()
          return Token { type: TokenType::AND, value: "&&", line: start_line, column: start_col }
        }

        if two_char == "||" {
          advance()
          advance()
          return Token { type: TokenType::OR, value: "||", line: start_line, column: start_col }
        }

        if two_char == "::" {
          advance()
          advance()
          return Token { type: TokenType::DOUBLE_COLON, value: "::", line: start_line, column: start_col }
        }

        if two_char == ".." {
          advance()
          advance()
          return Token { type: TokenType::DOTDOT, value: "..", line: start_line, column: start_col }
        }

        # Single-character tokens
        advance()

        if ch == "{" {
          return Token { type: TokenType::LBRACE, value: "{", line: start_line, column: start_col }
        }
        if ch == "}" {
          return Token { type: TokenType::RBRACE, value: "}", line: start_line, column: start_col }
        }
        if ch == "(" {
          return Token { type: TokenType::LPAREN, value: "(", line: start_line, column: start_col }
        }
        if ch == ")" {
          return Token { type: TokenType::RPAREN, value: ")", line: start_line, column: start_col }
        }
        if ch == "[" {
          return Token { type: TokenType::LBRACKET, value: "[", line: start_line, column: start_col }
        }
        if ch == "]" {
          return Token { type: TokenType::RBRACKET, value: "]", line: start_line, column: start_col }
        }
        if ch == "," {
          return Token { type: TokenType::COMMA, value: ",", line: start_line, column: start_col }
        }
        if ch == ":" {
          return Token { type: TokenType::COLON, value: ":", line: start_line, column: start_col }
        }
        if ch == "." {
          return Token { type: TokenType::DOT, value: ".", line: start_line, column: start_col }
        }
        if ch == "=" {
          return Token { type: TokenType::ASSIGN, value: "=", line: start_line, column: start_col }
        }
        if ch == "+" {
          return Token { type: TokenType::PLUS, value: "+", line: start_line, column: start_col }
        }
        if ch == "-" {
          return Token { type: TokenType::MINUS, value: "-", line: start_line, column: start_col }
        }
        if ch == "*" {
          return Token { type: TokenType::STAR, value: "*", line: start_line, column: start_col }
        }
        if ch == "/" {
          return Token { type: TokenType::SLASH, value: "/", line: start_line, column: start_col }
        }
        if ch == "%" {
          return Token { type: TokenType::PERCENT, value: "%", line: start_line, column: start_col }
        }
        if ch == "<" {
          if peek(0) == "<" {
            advance()
            return Token { type: TokenType::SHIFT_LEFT, value: "<<", line: start_line, column: start_col }
          }
          return Token { type: TokenType::LT, value: "<", line: start_line, column: start_col }
        }
        if ch == ">" {
          if peek(0) == ">" {
            advance()
            return Token { type: TokenType::SHIFT_RIGHT, value: ">>", line: start_line, column: start_col }
          }
          return Token { type: TokenType::GT, value: ">", line: start_line, column: start_col }
        }
        if ch == "!" {
          return Token { type: TokenType::NOT, value: "!", line: start_line, column: start_col }
        }
        if ch == "@" {
          return Token { type: TokenType::AT, value: "@", line: start_line, column: start_col }
        }
        if ch == ";" {
          return Token { type: TokenType::SEMICOLON, value: ";", line: start_line, column: start_col }
        }
        if ch == "|" {
          return Token { type: TokenType::PIPE, value: "|", line: start_line, column: start_col }
        }
        if ch == "&" {
          return Token { type: TokenType::AMPERSAND, value: "&", line: start_line, column: start_col }
        }

        # Unknown character - emit error
        state.error_count = state.error_count + 1
        emit compilation_error {
          stage: "lexer",
          message: format("Unexpected character: '{}'", ch),
          line: start_line,
          column: start_col
        }

        return Token { type: TokenType::ERROR, value: ch, line: start_line, column: start_col }
      }

      # Helper: check if character is a digit
      rule is_digit(ch: string) -> boolean {
        return ch == "0" || ch == "1" || ch == "2" || ch == "3" || ch == "4" ||
               ch == "5" || ch == "6" || ch == "7" || ch == "8" || ch == "9"
      }

      # Helper: check if character is a hex digit (0-9, a-f, A-F)
      rule is_hex_digit(ch: string) -> boolean {
        return is_digit(ch) ||
               ch == "a" || ch == "b" || ch == "c" || ch == "d" || ch == "e" || ch == "f" ||
               ch == "A" || ch == "B" || ch == "C" || ch == "D" || ch == "E" || ch == "F"
      }

      # Helper: check if character is alphabetic or underscore
      rule is_alpha_or_underscore(ch: string) -> boolean {
        return ch == "_" ||
               (ch >= "a" && ch <= "z") ||
               (ch >= "A" && ch <= "Z")
      }

      # Helper: check if character is alphanumeric or underscore
      rule is_alnum_or_underscore(ch: string) -> boolean {
        return is_alpha_or_underscore(ch) || is_digit(ch)
      }
    }
