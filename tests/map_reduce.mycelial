# Map-Reduce Pattern in Mycelial
# Demonstrates data parallelism: split work, process in parallel, aggregate results

network MapReduce {
  frequencies {
    batch {
      id: u32
      data: vec<i64>
    }

    chunk {
      task_id: u32
      partition: u32
      values: vec<i64>
    }

    partial_sum {
      task_id: u32
      worker_id: u32
      sum: i64
    }

    final_result {
      task_id: u32
      total_sum: i64
      count: u32
    }
  }

  hyphae {
    # Mapper: splits input into chunks and distributes
    hyphal mapper {
      state {
        partitions: u32 = 3
      }

      on signal(batch, b) {
        # Divide data into chunks
        let chunk_size = len(b.data) / state.partitions

        for i in 0..state.partitions {
          let start = i * chunk_size
          let end = (i + 1) * chunk_size
          let chunk_data = b.data[start..end]

          emit chunk {
            task_id: b.id,
            partition: i,
            values: chunk_data
          }
        }
      }
    }

    # Reducer: sums a chunk
    hyphal reducer {
      state {
        worker_id: u32 = 0
      }

      on signal(chunk, c) {
        let sum = sum(c.values)

        emit partial_sum {
          task_id: c.task_id,
          worker_id: state.worker_id,
          sum: sum
        }
      }
    }

    # Aggregator: collects partial results and produces final result
    hyphal aggregator {
      state {
        partial_results: map<u32, vec<i64>> = {}
        expected_partitions: u32 = 3
      }

      on signal(partial_sum, ps) {
        # Store this partial result
        if !state.partial_results.contains_key(ps.task_id) {
          state.partial_results[ps.task_id] = []
        }

        state.partial_results[ps.task_id].push(ps.sum)

        # Check if we have all partitions
        if len(state.partial_results[ps.task_id]) == state.expected_partitions {
          let total = sum(state.partial_results[ps.task_id])

          emit final_result {
            task_id: ps.task_id,
            total_sum: total,
            count: state.expected_partitions
          }

          state.partial_results.remove(ps.task_id)
        }
      }
    }
  }

  topology {
    fruiting_body input
    fruiting_body output

    spawn mapper as MAP
    spawn reducer as R1
    spawn reducer as R2
    spawn reducer as R3
    spawn aggregator as AGG

    # Input to mapper
    socket input -> MAP (frequency: batch)

    # Mapper distributes chunks to reducers
    socket MAP -> R1 (frequency: chunk)
    socket MAP -> R2 (frequency: chunk)
    socket MAP -> R3 (frequency: chunk)

    # Reducers send partial results to aggregator
    socket R1 -> AGG (frequency: partial_sum)
    socket R2 -> AGG (frequency: partial_sum)
    socket R3 -> AGG (frequency: partial_sum)

    # Aggregator sends final result to output
    socket AGG -> output (frequency: final_result)
  }
}

# EXAMPLE FLOW:
#   INPUT:  batch(id=1, data=[1,2,3,4,5,6,7,8,9])
#
#   MAP: Splits into 3 chunks
#     chunk(1, 0, [1,2,3])
#     chunk(1, 1, [4,5,6])
#     chunk(1, 2, [7,8,9])
#
#   REDUCE: Each worker sums its chunk
#     partial_sum(1, 0, 6)     # 1+2+3
#     partial_sum(1, 1, 15)    # 4+5+6
#     partial_sum(1, 2, 24)    # 7+8+9
#
#   AGGREGATE: Sums the partial results
#     final_result(1, 45, 3)   # 6+15+24=45
#
#   OUTPUT: final_result(id=1, total_sum=45)
